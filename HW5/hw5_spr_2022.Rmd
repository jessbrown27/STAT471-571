---
title: "Modern Data Mining - HW 5"
author:
- Jessica Brown
- Sarah Hayward
- Annie Vo
date: 'Due: 11:59Pm,  4/10, 2022'
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=6, warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, ISLR, rpart, rattle, pROC, partykit, ggplot2, glmnet, leaps, dplyr, keras, neuralnet, imager, ranger, tm, lubricate, nnet, data.table, wordcloud, sparsepca)
```




# Overview

For the purpose of predictions, a model free approach could be beneficial. A binary decision tree is the simplest, still interpretable and often provides insightful information between predictors and responses. To improve the predictive power we would like to aggregate many equations, especially uncorrelated ones. One clever way to have many free samples is to take bootstrap samples. For each bootstrap sample we  build a random tree by taking a randomly chosen number of variables to be split at each node. We then take average of all the random bootstrap trees to have our final prediction equation. This is RandomForest. 

Ensemble method can be applied broadly: simply take average or weighted average of many different equations. This may beat any single equation in your hand.


All the methods covered can handle both continuous responses as well as categorical response with multiple levels (not limited to binary response.)


## Objectives


- Understand trees
    + single tree/displaying/pruning a tree
    + RandomForest
    + Ensemble idea

- R functions/Packages
    + `tree`, `RandomForest`, `ranger`
    
- Json data format

- text mining
    + bag of words
  

Data needed:

+ `IQ.Full.csv`
+ `yelp_review_20k.json`

# Problem 0: Lectures

Please study all three lectures. Understand the main elements in each lecture and be able to run and compile the lectures

+ textmining
+ trees
+ boosting




# Problem 1: IQ and successes

## Background: Measurement of Intelligence 

Case Study:  how intelligence relates to one's future successes?

**Data needed: `IQ.Full.csv`**

ASVAB (Armed Services Vocational Aptitude Battery) tests have been used as a screening test for those who want to join the army or other jobs. 

Our data set IQ.csv is a subset of individuals from the 1979 National Longitudinal Study of 
Youth (NLSY79) survey who were re-interviewed in 2006. Information about family, personal demographic such as gender, race and education level, plus a set of ASVAB (Armed Services Vocational Aptitude Battery) test scores are available. It is STILL used as a screening test for those who want to join the army! ASVAB scores were 1981 and income was 2005. 

**Our goals:** 

+ Is IQ related to one's successes measured by Income?
+ Is there evidence to show that Females are under-paid?
+ What are the best possible prediction models to predict future income? 


**The ASVAB has the following components:**

+ Science, Arith (Arithmetic reasoning), Word (Word knowledge), Parag (Paragraph comprehension), Numer (Numerical operation), Coding (Coding speed), Auto (Automative and Shop information), Math (Math knowledge), Mechanic (Mechanic Comprehension) and Elec (Electronic information).
+ AFQT (Armed Forces Qualifying Test) is a combination of Word, Parag, Math and Arith.
+ Note: Service Branch requirement: Army 31, Navy 35, Marines 31, Air Force 36, and Coast Guard 45,(out of 100 which is the max!) 

**The detailed variable definitions:**

Personal Demographic Variables: 

 * Race: 1 = Hispanic, 2 = Black, 3 = Not Hispanic or Black
 * Gender: a factor with levels "female" and "male"
 * Educ: years of education completed by 2006
 
Household Environment: 
 
* Imagazine: a variable taking on the value 1 if anyone in the respondent’s household regularly read
	magazines in 1979, otherwise 0
* Inewspaper: a variable taking on the value 1 if anyone in the respondent’s household regularly read
	newspapers in 1979, otherwise 0
* Ilibrary: a variable taking on the value 1 if anyone in the respondent’s household had a library card
	in 1979, otherwise 0
* MotherEd: mother’s years of education
* FatherEd: father’s years of education

Variables Related to ASVAB test Scores in 1981 (Proxy of IQ's)

* AFQT: percentile score on the AFQT intelligence test in 1981 
* Coding: score on the Coding Speed test in 1981
* Auto: score on the Automotive and Shop test in 1981
* Mechanic: score on the Mechanic test in 1981
* Elec: score on the Electronics Information test in 1981

* Science: score on the General Science test in 1981
* Math: score on the Math test in 1981
* Arith: score on the Arithmetic Reasoning test in 1981
* Word: score on the Word Knowledge Test in 1981
* Parag: score on the Paragraph Comprehension test in 1981
* Numer: score on the Numerical Operations test in 1981

Variable Related to Life Success in 2006

* Income2005: total annual income from wages and salary in 2005. We will use a natural log transformation over the income.


**Note: All the Esteem scores shouldn't be used as predictors to predict income**

## 1. EDA: Some cleaning work is needed to organize the data. 

+ The first variable is the label for each person. Take that out.
+ Set categorical variables as factors. 
+ Make log transformation for Income and take the original Income out
+ Take the last person out of the dataset and label it as **Michelle**. 
+ When needed, split data to three portions: training, testing and validation (70%/20%/10%)
  - training data: get a fit
  - testing data: find the best tuning parameters/best models
  - validation data: only used in your final model to report the accuracy. 

```{r}
iq <- read.csv("IQ.Full.csv")
#removing first variable
iq <- iq[,-1]

#setting categorical as factors
iq$Gender <- as.factor(iq$Gender)
iq$Race <- as.factor(iq$Race)
iq$Imagazine <- as.factor(iq$Imagazine)
iq$Inewspaper <- as.factor(iq$Inewspaper)
iq$Ilibrary <- as.factor(iq$Ilibrary)

#setting income as log
iq$Income2005 <- log(iq$Income2005)

#making last row Michelle and removing it
Michelle <- tail(iq, 1)
iq <- iq[-2584,]
```


```{r}
# Split the data: 
N <- nrow(iq)
n1 <- floor(.7*N)
n2 <- floor(.2*N)

set.seed(10)
# Split data to three portions of .7, .2 and .1 of data size N

idx_train <- sample(N, n1)
idx_no_train <- (which(! seq(1:N) %in% idx_train))
idx_test <- sample( idx_no_train, n2)
idx_val <- which(! idx_no_train %in% idx_test)
data.train <- iq[idx_train,]
data.test <- iq[idx_test,]
data.val <- iq[idx_val,]
```


## 2. Factors affect Income

We only use linear models to answer the questions below.

i. To summarize ASVAB test scores, create PC1 and PC2 of 10 scores of ASVAB tests and label them as
ASVAB_PC1 and ASVAB_PC2. Give a quick interpretation of each ASVAB_PC1 and ASVAB_PC2 in terms of the original 10 tests. 

```{r}
pca.all <- prcomp(iq[,  c(10:19)], scale=TRUE)
pca.all.loading <- data.frame(tests=row.names(pca.all$rotation), pca.all$rotation)
#setting the variables of PC1 and PC2
ASVAB_PC1 <- pca.all.loading$PC1
ASVAB_PC2 <- pca.all.loading$PC2
```

```{r}
#Analyzing PC1
pca.all.loading %>% select(tests, PC1) %>%
  arrange(-PC1)
```
For PC1, we see that that the loadings are roughly similar which means that it captures  the total scores for the test, scaled by the standard deviations. It is essentially a weighted average.

```{r}
#Analyzing PC2
pca.all.loading %>% select(tests, PC2) %>%
  arrange(-PC2)
```
We can see that PC2 measures the difference between the total of Coding ,Numer, Parag , Math and the total of Mechanic, Elec, and Auto.


ii. Is there any evidence showing ASVAB test scores in terms of ASVAB_PC1 and ASVAB_PC2, might affect the Income?  Show your work here. You may control a few other variables, including gender. 

```{r}
#creating ASVAB variables for the data set
iq$ASVAB_PC1 <- pca.all$x[,1]
iq$ASVAB_PC2 <- pca.all$x[,2]
```

```{r}
#create a linear regression of the PC1 and PC2 scores to see if they are statistically significant with regards to Income
fit1 <- lm(Income2005~ASVAB_PC1 + ASVAB_PC2 + Gender, iq)
summary(fit1)
fit2 <- lm(Income2005~Gender, iq)
anova(fit2, fit1)
```
In a linear model using ASVAB_PC1, ASVAB_PC2, and gender to predict Income, we can see that the PC scores are statistically significant and this may affect the income. We made a full model of the 3 variables and a reduced model with just gender to see if the PC scores made a difference and we got a P-value of < 2.2e-16 meaning that the PC scores are statistically significant when predicting income while holding gender.


iii. Is there any evidence to show that there is gender bias against either male or female in terms of income in the above model? 

```{r}
anova(fit1)
summary(fit1)
```
Here we see that gender is statistically significant in the model when predicted income. In this fit, female is considered the base with a coefficient of zero whereas if the person is male, the log value of the income is 0.671353 higher. Thus there is a bias toward males here in that males are more likely to have a higher income based on solely their gender. 


We next build a few models for the purpose of prediction using all the information available. From now on you may use the three data sets setting (training/testing/validation) when it is appropriate. 

## 3. Trees

i. fit1: tree(Income ~ Educ + Gender, data.train) with default set up 

    a) Display the tree
    
```{r}
fit1 <- tree(Income2005 ~ Educ + Gender, data.train)
# plot the tree
plot(fit1)
text(fit1, pretty = TRUE)   # plot method 1
```
    
    b) How many end nodes? Briefly explain how the estimation is obtained in each end nodes and describe the prediction equation

The fit1 tree has 4 end nodes. The first split is based on gender of the person. After that you split the Educ on the person at a level of 15.5 (years of education completed by 2006). The estimation of each of the end nodes was determined by taking the average of log of the Income based on each split. So the first end node took all females with educ less that 15.5 and took the average, and so on for the rest of the nodes. 

    c) Does it show interaction effect of Gender and Educ over Income?

I would say that the tree does show the interaction effect of Gender and Educ over Income. Although the split for Educ occurs at the same point. We can see that the different genders have different incomes based on their educ. Typically, when we create trees with multuple variables as predictors we see their interactions. 

    d) Predict Michelle's income
```{r}
predict(fit1, Michelle)
```

Michelle's prediced log Income in 9.940797.

ii. fit2: fit2 <- rpart(Income2005 ~., data.train, minsplit=20, cp=.009)

    a) Display the tree using plot(as.party(fit2), main="Final Tree with Rpart")
    
```{r}
fit2 <- rpart(Income2005 ~., data.train, minsplit=20, cp=.009)
plot(as.party(fit2), main="Final Tree with Rpart", pretty = TRUE)
```
    
    b) A brief summary of the fit2
```{r}
fit2.s <- summary(fit2)
```
We can see that in total there are 8 end nodes in the tree for fit2. We see that the first split is based on the gender of the person. From there the different variables that are used are slightly different. The females are split based on their math score and number of years educ. The males have more splits that the females and they are their arith score, years of educ, and esteem 10. 

    c) Compare testing errors between fit1 and fit2. Is the training error from fit2 always less than that from fit1? Is the testing error from fit2 always smaller than that from fit1? 
    
```{r}
test.error.fit1 <- sum((predict(fit1, data.test) - data.test$Income2005)^2)
test.error.fit2 <- sum((predict(fit2, data.test) - data.test$Income2005)^2)
test.error.fit1
test.error.fit2
```

The testing error for fit1 is 345.306 and the testing error for fit2 is 351.766. As we can see, the testing error from fit2 is larger than the testing error for fit1. We can see that the testing error from fit2 is not alwasy smaller than fit1 because that depends on the data that you use to train and test the models with. 

```{r}
fit1.s <- summary(fit1)
fit1.s$dev
fit2.s <- summary(fit2)
fit2.s$frame

fit2.t <- tree(fit2, mindev = 0.008) #making fit2 into a tree so it's easier to prune and determine deviance
fit2.ts <- summary(fit2.t)
fit2.ts$dev
```

If you add up the deviance for all of the end nodes for fit2, you get a training error of about 1383 whereas the training error for fit1 is about 1482. We expect that the RSS to be the smallest for the model using all of the predictors and so the training error for fit2 should always be smaller than that of fit1.

    d) You may prune the fit2 to get a tree with small testing error. 
    
```{r}
fit2.p <- prune.tree(fit2.t, best = 5)
plot(fit2.p)
text(fit2.p, pretty = TRUE)
sum((predict(fit2.p, data.test) - data.test$Income2005)^2)
```
The testing error for fit2 before we did any pruning is 351.766. Here we were able to determine after pruning the best tree has 5 end nodes and gives a testing error of 346.2485 whicih is smaller than we had for fit2.

iii. fit3: bag two trees

    a) Take 2 bootstrap training samples and build two trees using the 
    rpart(Income2005 ~., data.train.b, minsplit=20, cp=.009). Display both trees.
```{r}
par(mfrow=c(1, 2))
n=1808
set.seed(1)  
index1 <- sample(n, n, replace = TRUE)
data.train.b <- data.train[index1, ]
boot.1 <- rpart(Income2005 ~., data.train.b, minsplit=20, cp=.009) 
plot(boot.1)
title(main = "First bootstrap tree")
text(boot.1, pretty=0)

set.seed(2)
index1 <- sample(n, n, replace = TRUE)
data.train.b <- data.train[index1, ]
boot.2 <- rpart(Income2005 ~., data.train.b, minsplit=20, cp=.009) 
plot(boot.2)
title(main = "Second bootstrap tree")
text(boot.2, pretty=0)
```
    
    b) Explain how to get fitted values for Michelle by bagging the two trees obtained above. Do not use the predict(). 

In order to obtain the fitted values for Michelle, we need to take the average of the two trees above. So we would look at the values that Michelle has along the tree and fit the predicted values for each of the trees and then take the average of the 2 predicted values. 

    c) What is the testing error for the bagged tree. Is it guaranteed that the testing error by bagging the two tree always smaller that either single tree? 

```{r}
fit3.p <- (predict(boot.1, data.test) + predict(boot.2, data.test))/2
test.error.fit3 <- sum((fit3.p - data.test$Income2005)^2)
test.error.fit3
# set.seed(1)
# fit3 <- randomForest(data.train, mtry = 31, ntree = 2)
```
We get that the testing error for the bagged tree is  361.3871. As we can see, the testing error is not smaller than that of a single tree. This could be due to the randomness of the sample selected for each of the bootstrap samples. It could give a prediction that does not do a good job of predicting the income for the testing sample.    

iv. fit4: Build a best possible RandomForest

    a) Show the process how you tune mtry and number of trees. Give a very high level explanation how fit4 is built.

```{r}
#given mtry, we see the effect of ntree first
par(mfrow=c(1, 3))
fit.rf.500 <- randomForest(Income2005~., data.train, mtry=5, ntree=500)
plot(fit.rf.500, col="red", pch=16, type="p", 
     main="Plot for 500 trees, ")
fit.rf.250 <- randomForest(Income2005~., data.train, mtry=5, ntree=250)
plot(fit.rf.250, col="red", pch=16, type="p", 
     main="Plot for 250 trees, ")
fit.rf.100 <- randomForest(Income2005~., data.train, mtry=5, ntree=100)
plot(fit.rf.100, col="red", pch=16, type="p", 
     main="Plot for 100 trees, ")
```
We can see that the error for 500 and 250 trees is about the same so we will set ntree to 250.
Now we can determine the optimal mtry

```{r}
rf.error.p <- 1:31 
for (p in 1:31)  
{
  fit.rf <- randomForest(Income2005~., data.train, mtry=p, ntree=250)
  rf.error.p[p] <- fit.rf$mse[250] 
}
rf.error.p  

plot(1:31, rf.error.p, pch=16,
     main = "Testing errors of mtry with 250 trees",
     xlab="mtry",
     ylab="OOB mse of mtry")
lines(1:31, rf.error.p)
```
The recommended mtry is p/3 so in our case 31/3 = 10 or 11. And we see that the OOB mse of mtry for 10 is pretty good so we will select 10 to use for our model.

```{r}
fit4 <- randomForest(Income2005~., data.train, mtry=10, ntree=250)
plot(fit4)
fit4$mse[250]
```

We were able to tune the values of mry and ntree by fixing one of the values and then lookinig at plots associated with the errors for the variables we were changing. First, we fixed mtry = 5 and changed the values of ntrees. We tested ntrees = 100, 250, and 500 to see which one would have the smallest error based on the graph. We determined that the errors for ntrees between 250 and 500 were approxiamately the same so we were able to set ntree=250. Then we tuned mtry to determine the best error for that variables. Typically we use p/31 which in our case would give us 31/10 which is about 10. We also ran a loop that looked at all of the different values for mtry and plotted them. We saw that 10 had a pretty low error so we were able to select that as the mtry.
    
    b) Compare the oob errors form fit4 to the testing errors using your testing data. Are you convinced that oob errors estimate testing error reasonably well.
```{r}
fit.rf.testing <- randomForest(Income2005~., data.train, xtest=data.test[, -21],
                          ytest=data.test[,21], mtry=10, ntree=250)
par(mfrow=c(1, 2))
plot(fit.rf.testing$mse)
plot(fit4)
```
We can see that the testing errors seem to agree with what we found using OOB errors. We can look at a graph of them together to get more evidence. 

```{r}
plot(1:250, fit.rf.testing$mse, col="red", pch=16,
     xlab="number of trees",
     ylab="mse",
     main="mse's of RF: blue=oob errors, red=testing errors")
points(1:250, fit4$mse, col="blue", pch=16)
```
We can see that the OOB errors do a good job of estimating the testing errors. 
    
    c) What is the predicted value for Michelle?
    
```{r}
predict(fit4, Michelle)
```
The predicted log income using fit4 for Michelle is 9.687374.
    
v. Now you have built so many predicted models (fit1 through fit4 in this section). What about build a fit5 which bags fit1 through fit4. Does fit5 have the smallest testing error?

```{r}
fit1.p <- predict(fit1, data.test)
fit2.p <- predict(fit2, data.test)
fit4.p <- predict(fit4, data.test)

test.error.fit1
test.error.fit2
test.error.fit3
test.error.fit4 <- sum((fit4.p - data.test$Income2005)^2)
test.error.fit4


fit5 <- (fit1.p + fit2.p + fit3.p + fit4.p)/4
test.error.fit5 <- sum((fit5 - data.test$Income2005)^2)
test.error.fit5
```

Earlier we determined the testing errors for each of the different fits. 
fit1 = 345.306
fit2 = 351.766
fit3 = 361.3871
fit4 = 334.7837
We can see that fit5 has a testing error of 333.1914. Thus we can see that fit5 has the smallest testing error of the 5 different fits. 

vi.  Summarize the results and nail down one best possible final model you will recommend to predict income. Explain briefly why this is the best choice. Finally for the first time evaluate the prediction error using the validating data set. 

Overall, we created 5 different fits using the training set from the IQ data set. fit1 focused on just the variables gender and educ. fit2 was a tree created from all of the different variables in the data set. fit3 bagged two different bootstrap trees to get the prediction. fit4 was optimizing a random forest in which we used mtry=10 and ntrees=250. Finally, fit5 bagged the 4 different trees to get the final prediction. We compared all of the different testing errors for the tests and saw that fit5 had the lowest testing error, however, fit5 bagged the previous 4 different fits and since the testing error for fit4 was only slightly higher we are going to select fit4 as the final model since we made sure to optimize the MSE and OOB when creating that test as well as it having a low testing error. 

```{r}
sum((predict(fit4, data.val) - data.val$Income2005)^2)
```

We get that the prediction error for the validation data set is 58.71275. 

vii. Use your final model to predict Michelle's income. 

```{r}
predict(fit4, Michelle)
```
We get that Michelle's predict log income is 9.687374.
    
# Problem 2: Yelp challenge 2019

**Note:** This problem is rather involved. It covers essentially all the main materials we have done so far in this semester. It could be thought as a guideline for your final project if you want when appropriate. 

Yelp has made their data available to public and launched Yelp challenge. [More information](https://www.yelp.com/dataset/). It is unlikely we will win the $5,000 prize posted but we get to use their data for free. We have done a detailed analysis in our lecture. This exercise is designed for you to get hands on the whole process. 

For this case study, we downloaded the [data](https://www.yelp.com/dataset/download) and took a 20k subset from **review.json**. *json* is another format for data. It is flexible and commonly-used for websites. Each item/subject/sample is contained in a brace *{}*. Data is stored as **key-value** pairs inside the brace. *Key* is the counterpart of column name in *csv* and *value* is the content/data. Both *key* and *value* are quoted. Each pair is separated by a comma. The following is an example of one item/subject/sample.

```{json}
{
  "key1": "value1",
  "key2": "value2"
}
```


**Data needed:** yelp_review_20k.json available in Canvas.

**yelp_review_20k.json** contains full review text data including the user_id that wrote the review and the business_id the review is written for. Here's an example of one review.

```{json}
{
    // string, 22 character unique review id
    "review_id": "zdSx_SD6obEhz9VrW9uAWA",

    // string, 22 character unique user id, maps to the user in user.json
    "user_id": "Ha3iJu77CxlrFm-vQRs_8g",

    // string, 22 character business id, maps to business in business.json
    "business_id": "tnhfDv5Il8EaGSXZGiuQGg",

    // integer, star rating
    "stars": 4,

    // string, date formatted YYYY-MM-DD
    "date": "2016-03-09",

    // string, the review itself
    "text": "Great place to hang out after work: the prices are decent, and the ambience is fun. It's a bit loud, but very lively. The staff is friendly, and the food is good. They have a good selection of drinks.",

    // integer, number of useful votes received
    "useful": 0,

    // integer, number of funny votes received
    "funny": 0,

    // integer, number of cool votes received
    "cool": 0
}
```

## Goal of the study

The goals are 

1) Try to identify important words associated with positive ratings and negative ratings. Collectively we have a sentiment analysis.  

2) To predict ratings using different methods. 

## 1. JSON data and preprocessing data

i. Load *json* data

The *json* data provided is formatted as newline delimited JSON (ndjson). It is relatively new and useful for streaming.
```{json}
{
  "key1": "value1",
  "key2": "value2"
}
{
  "key1": "value1",
  "key2": "value2"
}
```

The traditional JSON format is as follows.
```{json}
[{
  "key1": "value1",
  "key2": "value2"
},
{
  "key1": "value1",
  "key2": "value2"
}]
```


We use `stream_in()` in the `jsonlite` package to load the JSON data (of ndjson format) as `data.frame`. (For the traditional JSON file, use `fromJSON()` function.)

```{r}
pacman::p_load(jsonlite)
yelp_data <- jsonlite::stream_in(file("data/yelp_review_20k.json"), verbose = F)
str(yelp_data)  

# different JSON format
# tmp_json <- toJSON(yelp_data[1:10,])
# fromJSON(tmp_json)
```

**Write a brief summary about the data:**

a) Which time period were the reviews collected in this data?
```{r}
yelp_data$cleaned_date <- as.Date(yelp_data$date)
print(min(yelp_data$cleaned_date)) 
print(max(yelp_data$cleaned_date))
```

The reviews were collected from 10/19/2004 to 10/04/2018. 

b) Are ratings (with 5 levels) related to month of the year or days of the week? Only address this through EDA please. 

```{r}
yelp_data$month <- as.factor(month(yelp_data$cleaned_date))
yelp_data %>% ggplot(aes(x = month, fill = as.factor(stars))) +
  geom_bar(stat="count", width=0.7) +
  theme_minimal() + ggtitle("Distribution of Month For Each Star Rating") + labs(fill = "stars")
```

```{r}
yelp_data$wday <- lubridate::wday(x = yelp_data$cleaned_date, label = TRUE)
yelp_data %>% ggplot(aes(x = wday, fill = as.factor(stars))) +
  geom_bar(stat="count", width=0.7) +
  theme_minimal() + ggtitle("Distribution of Weekday For Each Star Rating") + xlab("weekday") + labs(fill = "stars")
```


ii. Document term matrix (dtm)
 
 Extract document term matrix for texts to keep words appearing at least .5% of the time among all 20000 documents. Go through the similar process of cleansing as we did in the lecture. 

a) Briefly explain what does this matrix record? What is the cell number at row 100 and column 405? What does it represent?
 
```{r}
yelp.text <- yelp_data$text

corpus <- VCorpus(VectorSource(yelp.text))
corpus

# Converts all words to lowercase
corpus_clean <- tm_map(corpus, content_transformer(tolower))

# Removes common English stopwords (e.g. "with", "i")
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english"))

corpus_clean <- tm_map(corpus_clean, removePunctuation)

# Removes numbers
corpus_clean <- tm_map(corpus_clean, removeNumbers)

# Stem words
corpus_clean <- tm_map(corpus_clean, stemDocument, lazy = TRUE)

dtm <- DocumentTermMatrix(corpus_clean)

inspect(dtm)

threshold <- .005*length(corpus_clean)   # 0.5% of the total documents 
words.005 <- findFreqTerms(dtm, lowfreq=threshold)  # words appearing at least among 0.5% of the documents

dtm.005<- DocumentTermMatrix(corpus_clean, control = list(dictionary = words.005))
dtm.005
```

a) Briefly explain what does this matrix record? What is the cell number at row 100 and column 405? What does it represent?

This matrix records the frequency for each word in the corpus for each document. The rows in the matrix are the 20,000 documents and the columns are the words in our cleaned corpus corpus, with the cells storing the frequencies of the words in each document. 

```{r}
inspect(dtm.005[100,405])
```
The cell number at row 100 and column 405 is 0. This represents that the word "driver" does not appear in the 100th document but is still in our corpus because it appears at least 0.5% of the time among all of our documents.

b) What is the sparsity of the dtm obtained here? What does that mean?

The sparsity is 98% which means that 98% of the cells in our document term matrix have a value of 0.

iii. Set the stars as a two category response variable called rating to be “1” = 5,4 and “0”= 1,2,3. Combine the variable rating with the dtm as a data frame called data2. 

```{r}
yelp_data$rating = as.factor(ifelse(yelp_data$stars == 5 | yelp_data$stars == 4, 1, 0))
data2 <- data.frame(yelp_data$rating, as.matrix(dtm.005))
data2 <- data2 %>% rename(rating = yelp_data.rating)
```


## Analysis

Get a training data with 13000 reviews and the 5000 reserved as the testing data. Keep the rest (2000) as our validation data set. 

```{r}
set.seed(1) 
n <- nrow(data2)
sample.index <- sample(n, 13000)
data2.train <- data2[sample.index, ]
test_and_val <- data2[-sample.index, ]
n_tv <- nrow(test_and_val)
sample_tv <- sample(n_tv, 5000)
data2.test <- test_and_val[sample_tv, ]
data2.val <- test_and_val[-sample_tv, ]
dim(data2.train)
dim(data2.test)
dim(data2.val)
```


## 2. LASSO

i. Use the training data to get Lasso fit. Choose lambda.1se. Keep the result here.
```{r}
y <- data2.train$rating
X1 <- sparse.model.matrix(rating~., data = data2.train)[, -1]
set.seed(2)

result.lasso <- cv.glmnet(X1, y, alpha=.99, family="binomial")
plot(result.lasso)

# save results to result.lasso
# saveRDS(result.lasso, file="data/TextMining_lasso.RDS")
# result.lasso can be assigned back by 
# result.lasso <- readRDS("data/TextMining_lasso.RDS"
```
```{r}
# number of non-zero words picked up by LASSO when using lambda.1se
coef.1se <- coef(result.lasso, s="lambda.1se")  
coef.1se <- coef.1se[which(coef.1se !=0),] 
lasso.words <- rownames(as.matrix(coef.1se))[-1]
length(lasso.words)
lasso.words[1:10]
```

Since there are 438 words selected by lasso, we decided to just output a small sample of the words to get a sense of those selected. 

ii. Feed the output from Lasso above, get a logistic regression. 

```{r}
sel_cols <- c("rating", lasso.words)
data_sub <- data2.train %>% select(all_of(sel_cols))
result.glm <- glm(rating~., family=binomial, data_sub)
#save results
#saveRDS(result.glm, file = "data/TextMining_glm.RDS")
```

	
a) Pull out all the positive coefficients and the corresponding words. Rank the coefficients in a decreasing order. Report the leading 2 words and the coefficients. Describe briefly the interpretation for those two coefficients. 

```{r}
result.glm.coef <- coef(result.glm)
pos.coef <- result.glm.coef[which(result.glm.coef > 0)]
pos.coef <- pos.coef[-1]
sorted.pos <-  sort(pos.coef, decreasing = TRUE)
sorted.pos[1:2]
```

The leading 2 words are gem and bomb with coefficients of 2.440625 and 2.280553 respectively. These coefficients mean that the presence of the word gem in a review increases the probability that a review is "highly" rated (rating = 1) by 2.440625 on average and the presence of the word bomb in a review increases the probability that a review is "highly" rated (rating = 1) by 2.280553. 

b) Make a word cloud with the top 100 positive words according to their coefficients. Interpret the cloud briefly.

```{r}
pos.words <- names(sorted.pos)
cor.special <- brewer.pal(8, "Dark2")
wordcloud(pos.words[1:100], sorted.pos[1:100], colors=cor.special, ordered.colors=F)
```


c) Repeat i) and ii) for the bag of negative words.

```{r}
neg.coef <- result.glm.coef[which(result.glm.coef < 0)]
neg.coef <- neg.coef[-1]
sorted.neg <-  sort(neg.coef, decreasing = FALSE)
sorted.neg[1:2]
```

```{r}
neg.words <- names(sorted.neg)
wordcloud(neg.words[1:100], abs(sorted.neg[1:100]), colors = cor.special, ordered.colors = F, min.freq = abs(sorted.neg[100]))
```

d) Summarize the findings. 

iii. Using majority votes find the testing errors
	i) From Lasso fit in 3)
```{r}
# get majority vote labels
predict.lasso <- predict(result.lasso, as.matrix(data2.test[, -1]), type = "class", s = "lambda.1se")

# LASSO testing errors
mean(data2.test$rating != predict.lasso)
```

	ii) From logistic regression in 4)
```{r}
predict.glm <- predict(result.glm, data2.test, type = "response")
class.glm <- ifelse(predict.glm > .5, 1, 0)
mean(data2.test$rating != class.glm)
```
	
	
iii) Which one is smaller?
The logistic regression testing error is slightly lower than the testing error from the LASSO fit (0.131 vs. 0.1378). 

## 3. Random Forest  

i. Briefly summarize the method of Random Forest

From 1 to B, Random Forest takes a sample and builds a tree using that sample until n_min is met. The tree is built by randomly selecting m variables and finding the best split point for each (where the misclassification erors is minimized by majority vote). The node with the best variable and split point is split into two resulting in an end note with majority vote either 0 or 1. The leaves of the majority vote of the aggregated trees can provide an estimated prediction and the leaves of the sample proportion of 1's among aggregated trees can provide an estimated probability.

ii. Now train the data using the training data set by RF. Get the testing error of majority vote. Also explain how you tune the tuning parameters (`mtry` and `ntree`). 

```{r}
set.seed(1)
fit.rf <- randomForest(rating~., data2.train, mtry=3, ntree=50)
predicted_oob <- fit.rf$predicted
rf.test.err <- mean(data2.train$rating != predicted_oob)
```

The testing error is `r rf.test.err`. We ran the RF using fixed parameters of mtry=3 and and ntree=50. To tune the parameters, we would first look at the effect of ntree with a given mtry. Then we would fix that ntree and plot the OOB errors. The recommended mtry is p/3.

## 4. Boosting 

To be determined. 


## 5.  PCA first

i. Perform PCA (better to do sparse PCA) for the input matrix first. Decide how many PC's you may want to take and why.

```{r}
data2.train.num <- data2.train
data2.train.num$rating <- as.numeric(data2.train.num$rating)
pc <- sparsepca::spca(data2.train.num, scale=TRUE)
plot(summary(pc)$importance[2, ],  # PVE
     ylab="PVE",
     xlab="Number of PC's",
     pch = 16, 
     main="Scree Plot of PVE")
```

We would want to take around 50 PCs because that is where the elbow is (per the elbow rule).

ii. Pick up one of your favorite method above and build the predictive model with PC's. Say you use RandomForest.

```{r}
pc.loading <- pc$rotation
temp <- pc$x[, 1:50]
data3 <- data.frame(data2.train,temp)

fit.rf.2 <- randomForest(rating~., data3, mtry=3, ntree=50)
predicted_oob.2 <- fit.rf.2$predicted
rf.test.err.2 <- mean(data3$rating != predicted_oob.2)
```

iii. What is the testing error? Is this testing error better than that obtained using the original x's? 

The testing error is `r rf.test.err.2` which is better than that obtained using the original x's.

## 6. Ensemble model

i. Take average of some of the  models built above (also try all of them) and this gives us the fifth model. Report it's testing error. (Do you have more models to be bagged, try it.)

```{r}
predicted_average_lasso <- round(((class.glm + as.numeric(predict.lasso)) / 2), 0)
predicted_average_err_lasso <- mean(data3$rating != predicted_average_lasso)

#as.numeric added 1 to the predictions so we subtracted 2 to combat that
predicted_average_forest <- round(((as.numeric(predicted_oob) + as.numeric(predicted_oob.2) - 2) / 2), 0)
predicted_average_err_forest <- mean(data3$rating != predicted_average_forest)

predicted_average_three <- round(((as.numeric(predicted_oob) + as.numeric(predicted_oob.2) - 2 + class.glm) / 3), 0)
predicted_average_err_three <- mean(data3$rating != predicted_average_three)

predicted_average_all <- round(((as.numeric(predicted_oob) + as.numeric(predicted_oob.2) - 2 + class.glm + as.numeric(predict.lasso)) / 4), 0)
predicted_average_err_all <- mean(data3$rating != predicted_average_all)
```

Our fifth chosen is the one that averages the random forest models. The testing error is `r predicted_average_err_forest`.

## 7. Final model

Which classifier(s) seem to produce the least testing error? Are you surprised? Report the final model and accompany the validation error. Once again this is THE only time you use the validation data set.  For the purpose of prediction, comment on how would you predict a rating if you are given a review (not a tm output) using our final model? 

The classifier that produces the least testing error was the logistic regression using the variables selected by LASSO. We are surprised by this because we thought the ensemble models would perform better because they pull together results of different models and because the logistic regression is a more simple model. Our








