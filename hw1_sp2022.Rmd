---
title: " Modern Data Mining, HW 1"
author:
- Sarah Hayward
- Annie Vo
- Jessica Brown
date: 'Due: 11:59PM,  Jan. 30th, 2021'
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results = "hide", fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output
# check if you have ISLR package, if not, install it
if(!require('pacman')) {install.packages('pacman')}
pacman::p_load(ISLR, readxl, tidyverse, magrittr, dplyr, ggplot2)
```


\pagebreak

# Overview

This is a fast-paced course that covers a lot of material. There will be a large amount of references. You may need to do your own research to fill in the gaps in between lectures and homework/projects. It is impossible to learn data science without getting your hands dirty. Please budget your time evenly. Last-minute work ethic will not work for this course. 

Homework in this course is different from your usual homework assignment as a typical student. Most of the time, they are built over real case studies.  While you will be applying methods covered in lectures, you will also find that extra teaching materials appear here.  The focus will be always on the goals of the study, the usefulness of the data gathered, and the limitations in any conclusions you may draw. Always try to challenge your data analysis in a critical way. Frequently, there are no unique solutions. 

Case studies in each homework can be listed as your data science projects (e.g. on your CV) where you see fit. 



## Objectives 

- Get familiar with `R-studio` and `RMarkdown`
- Hands-on R 
- Learn data science essentials 
    - gather data
    - clean data
    - summarize data 
    - display data
    - conclusion
- Packages
    - `dplyr`
    - `ggplot`

##  Instructions

- **Homework assignments can be done in a group consisting of up to three members**. Please find your group members as soon as possible and register your group on our Canvas site.

- **All work submitted should be completed in the R Markdown format.** You can find a cheat sheet for R Markdown [here](https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf). For those who have never used it before, we urge you to start this homework as soon as possible. 

- **Submit the following files, one submission for each group:**  (1) Rmd file, (2) a compiled PDF or HTML version, and (3) all necessary data files if different from our source data. You may directly edit this .rmd file to add your answers. If you intend to work on the problems separately within your group, compile your answers into one Rmd file before submitting. We encourage that you at least attempt each problem by yourself before working with your teammates. Additionally, ensure that you can 'knit' or compile your Rmd file. It is also likely that you need to configure Rstudio to properly convert files to PDF. [**These instructions**](http://kbroman.org/knitr_knutshell/pages/latex.html#converting-knitrlatex-to-pdf) might be helpful.

- In general, be as concise as possible while giving a fully complete answer to each question. All necessary datasets are available in this homework folder on Canvas. Make sure to document your code with comments (written on separate lines in a code chunk using a hashtag `#` before the comment) so the teaching fellows can follow along. R Markdown is particularly useful because it follows a 'stream of consciousness' approach: as you write code in a code chunk, make sure to explain what you are doing outside of the chunk. 

- A few good or solicited submissions will be used as sample solutions. When those are released, make sure to compare your answers and understand the solutions.


## Review materials

- Study Advanced R Tutorial (to include `dplyr` and `ggplot`)
- Study lecture 1: Data Acquisition and EDA


# Case study 1: Audience Size

How successful is the Wharton Talk Show [Business Radio Powered by the Wharton School](https://businessradio.wharton.upenn.edu/)  


**Background:** Have you ever listened to [SiriusXM](https://www.siriusxm.com/)? Do you know there is a **Talk Show** run by Wharton professors in Sirius Radio?  Wharton launched a talk show called [Business Radio Powered by the Wharton School](https://businessradio.wharton.upenn.edu/) through the Sirius Radio station in January of 2014. Within a short period of time the general reaction seemed to be overwhelmingly positive. To find out the audience size for the show, we designed a survey and collected a data set via MTURK in May of 2014. Our goal was to **estimate the audience size**. There were 51.6 million Sirius Radio listeners then. One approach is to estimate the proportion of the Wharton listeners to that of the Sirius listeners, $p$, so that we will come up with an audience size estimate of approximately 51.6 million times $p$. 

To do so, we launched a survey via Amazon Mechanical Turk ([MTurk](https://www.mturk.com/)) on May 24, 2014 at an offered price of \$0.10 for each answered survey.  We set it to be run for 6 days with a target maximum sample size of 2000 as our goal. Most of the observations came in within the first two days. The main questions of interest are "Have you ever listened to Sirius Radio" and "Have you ever listened to Sirius Business Radio by Wharton?". A few demographic features used as control variables were also collected; these include Gender, Age and Household Income.  

We requested that only people in United States answer the questions. Each person can only fill in the questionnaire once to avoid duplicates. Aside from these restrictions, we opened the survey to everyone in MTurk with a hope that the sample would be more randomly chosen. 

The raw data is stored as `Survey_results_final.csv` on Canvas.

```{r}
survey_data = read_csv('Survey_results_final.csv')
```


## Data preparation

i. We need to clean and select only the variables of interest. 

Select only the variables Age, Gender, Education Level, Household Income in 2013, Sirius Listener?, Wharton Listener? and Time used to finish the survey.

Change the variable names to be "age", "gender", "education", "income", "sirius", "wharton", "worktime".

```{r}
#get selected columns and change datatypes of numeric columnss
cleaned_survey_data = survey_data %>% select("Answer.Age", "Answer.Gender", "Answer.Education", "Answer.HouseHoldIncome", "Answer.Sirius Radio", "Answer.Wharton Radio", "WorkTimeInSeconds") %>% rename(c(age = Answer.Age, gender = Answer.Gender, education = Answer.Education, income = Answer.HouseHoldIncome, sirius = "Answer.Sirius Radio", wharton = "Answer.Wharton Radio", worktime = WorkTimeInSeconds)) %>% mutate(across(c(age, worktime), as.integer))
```


ii. Handle missing/wrongly filled values of the selected variables

As in real world data with user input, the data is incomplete, with missing values, and has incorrect responses. There is no general rule for dealing with these problems beyond “use common sense.” In whatever case, explain what the problems were and how you addressed them. Be sure to explain your rationale for your chosen methods of handling issues with the data. Do not use Excel for this, however tempting it might be.

Tip: Reflect on the reasons for which data could be wrong or missing. How would you address each case? For this homework, if you are trying to predict missing values with regression, you are definitely overthinking. Keep it simple.

```{r}
#get unique values for each column
lapply(cleaned_survey_data, unique)
```


**Answer:** We found that there were 35 rows with at least one missing value or unspecified answer (exclusive or). For most columns, this correlated to an NA value, except for education there are "select one" responses which likely indicate a drop down option where the respondant did not answer the questio. For this small sample of surveys not completely filled out, we opted to remove them to preserve the integrity of the data. 

```{r}
print(nrow(cleaned_survey_data[xor(rowSums(is.na(cleaned_survey_data)) > 0, cleaned_survey_data$education == "select one"), ]))
```

```{r}
#drop missing values
final_survey_data = cleaned_survey_data[rowSums(is.na(cleaned_survey_data)) == 0 & cleaned_survey_data$education != "select one", ]
```

From the intial data, we can also see that there are a few questionable/wrong age values, namely 4 and 223 which were likely filled in incorrectly. We can see that these values consist of only 2 rows, one for each value, after we removed the missing data. The data also affirms that the row for the 4-year-old is incorrect given that there's no way a toddler could complete a bachelor's or 4-year degree. 

```{r}
final_survey_data %>% filter(age == 4 | age == 223)
```
```{r}
final_survey_data = final_survey_data %>% filter(age != 4 & age != 223)
```

Overall, we ended up removing only 37 rows from our dataset. 

iii. Brief summary 

Write a brief report to summarize all the variables collected. Include both summary statistics (including sample size) and graphical displays such as histograms or bar charts where appropriate. Comment on what you have found from this sample. (For example - it's very interesting to think about why would one work for a job that pays only 10cents/each survey? Who are those survey workers? The answer may be interesting even if it may not directly relate to our goal.)

We can first determine the sample size of our data and see that we are considering 1,764 survey results.

```{r}
cat("Sample size: ", nrow(cleaned_survey_data))
```

We can then delve into the other summary statistics of the data. Since all of our columns contain categorical data, worktime is the only numerical and continuous 

```{r}
summary(cleaned_survey_data)
```

```{r}

```


## Sample properties

The population from which the sample is drawn determines where the results of our analysis can be applied or generalized. We include some basic demographic information for the purpose of identifying sample bias, if any exists. Combine our data and the general population distribution in age, gender and income to try to characterize our sample on hand.

i. Does this sample appear to be a random sample from the general population of the USA?
ii. Does this sample appear to be a random sample from the MTURK population?

Note: You can not provide evidence by simply looking at our data here. For example, you need to find distribution of education in our age group in US to see if the two groups match in distribution. You may need to gather some background information about the MTURK population to have a slight sense if this particular sample seem to a random sample from there... Please do not spend too much time gathering evidence. 

## Final estimate

Give a final estimate of the Wharton audience size in January 2014. Assume that the sample is a random sample of the MTURK population, and that the proportion of Wharton listeners vs. Sirius listeners in the general population is the same as that in the MTURK population. Write a brief executive summary to summarize your findings and how you came to that conclusion.

To be specific, you should include:

1. Goal of the study
2. Method used: data gathering, estimation methods
3. Findings
4. Limitations of the study. 


## New task

Now suppose you are asked to design a study to estimate the audience size of Wharton Business Radio Show as of today: You are given a budget of $1000. You need to present your findings in two months. 

Write a proposal for this study which includes:

1. Method proposed to estimate the audience size.
2. What data should be collected and where it should be sourced from.
Please fill in the google form to list your platform where surveys will be launched and collected [HERE](https://forms.gle/8SmjFQ1tpqr6c4sa8) 

A good proposal will give an accurate estimation with the least amount of money used. 





# Case study 2: Women in Science


Are women underrepresented in science in general? How does gender relate to the type of educational degree pursued? Does the number of higher degrees increase over the years? In an attempt to answer these questions, we assembled a data set (`WomenData_06_16.xlsx`) from [NSF](https://ncses.nsf.gov/pubs/nsf19304/digest/field-of-degree-women) about various degrees granted in the U.S. from 2006 to 2016. It contains the following variables: Field (Non-science-engineering (`Non-S&E`) and sciences (`Computer sciences`, `Mathematics and statistics`, etc.)), Degree (`BS`, `MS`, `PhD`), Sex (`M`, `F`), Number of degrees granted, and Year.

Our goal is to answer the above questions only through EDA (Exploratory Data Analyses) without formal testing. We have provided sample R-codes in the appendix to help you if needed. 


## Data preparation  

1. Understand and clean the data

Notice the data came in as an Excel file. We need to use the package `readxl` and the function `read_excel()` to read the data `WomenData_06_16.xlsx` into R. 


```{r}
install.packages("readxl")
library("readxl")
```


i. Read the data into R.

```{r}
degree_data = read_excel('WomenData_06_16.xlsx')
```

ii. Clean the names of each variables. (Change variable names to  `Field`,`Degree`, `Sex`, `Year` and `Number` )

```{r}
degree_data <- degree_data %>% rename(Field = `Field and sex`, Number = `Degrees Awarded`)
names(degree_data)
```

iii. Set the variable natures properly. 

```{r}
str(degree_data)
summary(degree_data)
degree_data[,c('Degree','Sex')] <- lapply(degree_data[,c('Degree','Sex')], as.factor)
degree_data[,c('Year','Number')] <- lapply(degree_data[,c('Year','Number')], as.integer)
str(degree_data)
summary(degree_data)
```

iv. Any missing values?

```{r}
degree_data$Field
```

There are no missing values

2. Write a summary describing the data set provided here. 

```{r}
summary(degree_data)
length(unique(degree_data$Field))
```

i. How many fields are there in this data?

There are 10 distinct fields in this data.

ii. What are the degree types? 

The degree types are BS, MS and PhD.

iii. How many year's statistics are being reported here? 

There are 10 years of statistics being reported from 2006-2016.

## BS degrees in 2015

Is there evidence that more males are in science-related fields vs `Non-S&E`? Provide summary statistics and a plot which shows the number of people by gender and by field. Write a brief summary to describe your findings.

```{r}
degree_data %>%
  mutate(Field = ifelse(Field!="Non-S&E" , "S&E", "Non-S&E")) %>%
  ggplot(aes(x = Sex, y = Number, fill = ScienceField)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(scales = "free_y") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  ggtitle("Degrees granted across fields by degree and gender") 
```

## EDA bringing type of degree, field and gender in 2015

Describe the number of people by type of degree, field, and gender. Do you see any evidence of gender effects over different types of degrees? Again, provide graphs to summarize your findings.

## EDA bring all variables 

In this last portion of the EDA, we ask you to provide evidence numerically and graphically: Do the number of  degrees change by gender, field, and time? 

## Women in Data Science

Finally, is there evidence showing that women are underrepresented in data science? Data science is an interdisciplinary field of computer science, math, and statistics. You may include year and/or degree.

## Final brief report

Summarize your findings focusing on answering the questions regarding if we see consistent patterns that more males pursue science-related fields. Any concerns with the data set? How could we improve on the study?

## Appendix

To help out, we have included some R-codes here as references. You should make your own chunks filled with texts going through each items listed above. Make sure to hide the unnecessary outputs/code etc. 

1. Clean data

```{r data wrangling, echo = FALSE, warning = FALSE}
# For the demonstration purpose, we show this R-chunk by taking echo=TRUE
# In your final report you should hide all the R-chunks to keep your report flowing well.
wsci <- read_excel("WomenData_06_16.xlsx")
names(wsci)
head(wsci)
#change names
wsci %<>% 
  rename(Field = 'Field and sex')
# set the field, degree and sex as factors
wsci %<>% 
  mutate( Field = as.factor(Field))
wsci
```


```{r, echo=FALSE}
# wsci %<>%
#   rename(Field = "Field and sex",
#          Number = "Degrees Awarded") %>%
#   mutate(Field = as.factor(Field),
#          Degree = as.factor(Degree),
#          Sex = as.factor(Sex))

```


2. A number of sample analyses 

```{r eval = FALSE, echo = FALSE}
wsci %>%  # to get the average number of ppl by gender
  group_by(Field, Sex) %>%
  summarise(deg = mean(Number))

wsci %>%
  filter(Year == 2007) %>%
  ggplot(aes(x = Field, y = Number, fill = Sex)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(Degree~., scales = "free_y") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  ggtitle("Degrees granted across fields by degree and gender") 

wsci %>%
  mutate(SE = ifelse(Field!="Non-S&E" , "S&E", "Non-S&E")) %>%
  group_by(SE, Sex) %>%
  summarise(SE_number = sum(Number)) %>%
  ggplot(aes(x = SE, y = SE_number, fill = Sex)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.y = element_text(angle = 60)) +
  ggtitle("Degrees granted by S&E vs non-S&E by gender")

wsci %>%
  mutate(SE = ifelse(Field!="Non-S&E" , "S&E", "Non-S&E")) %>%
  group_by(SE, Sex, Year) %>%
  summarise(SE_number = sum(Number)) %>%
  group_by(SE, Year) %>%
  mutate(ratio = SE_number / sum(SE_number)) %>%
  filter(Sex == "Female") %>%
  ggplot(aes(x = Year, y = ratio, color = SE)) +
  geom_point() + geom_line() +
  ggtitle("Female proportion in SE/non-SE across year")

wsci %>%
  mutate(SE = ifelse(Field!="Non-S&E" , "S&E", "Non-S&E")) %>%
  group_by(SE, Sex, Year, Degree) %>%
  summarise(SE_number = sum(Number)) %>%
  group_by(SE, Year, Degree) %>%
  mutate(ratio = SE_number / sum(SE_number)) %>%
  filter(Sex == "Female") %>%
  ggplot(aes(x = Year, y = ratio, color = SE)) +
  geom_point() + geom_line() +
  facet_grid(~Degree)+
  ggtitle("Female proportion in SE/non-SE across year by degree")


wsci %>%
  mutate(SE = ifelse(Field!="Non-S&E" , "S&E", "Non-S&E")) %>%
  group_by(SE, Sex, Year, Degree) %>%
  summarise(SE_number = sum(Number)) %>%
  ggplot(aes(x = Year, y = SE_number, fill = Sex)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(SE~Degree, scales = "free_y") +
  ggtitle("Degrees granted by sex, degree and SE")


wsci %>%
  mutate(SE = ifelse(Field!="Non-S&E" , "S&E", "Non-S&E")) %>%
  group_by(SE, Sex, Year, Degree) %>%
  summarise(SE_number = sum(Number)) %>%
  ggplot(aes(x = Year, y = SE_number, fill = Sex)) +
  geom_bar(stat = "identity", position = "fill") +
  facet_grid(SE~Degree, scales = "free_y") +
  ggtitle("Degrees granted proportion by sex across degree and SE")


wsci %>%
  filter(Field %in% c("Computer sciences", "Mathematics and statistics")) %>%
  ggplot(aes(x = Year, y = Number, fill = Sex)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(Field~Degree, scales = "free_y") +
  ggtitle("Degrees granted pr option by sex across degree and SE")


wsci %>%
  ggplot(aes(x = Year, y = Number, fill = Sex)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(Field~Degree, scales = "free_y") +
  ggtitle("Degrees granted proportion by sex across degree and SE")
```




# Case study 3: Major League Baseball

We would like to explore how payroll affects performance among Major League Baseball teams. The data is prepared in two formats record payroll, winning numbers/percentage by team from 1998 to 2014. 

Here are the datasets:

-`MLPayData_Total.csv`: wide format
-`baseball.csv`: long format

Feel free to use either dataset to address the problems. 

```{r, echo=FALSE}
baseball <- read_csv("baseball.csv")
payData <- read_csv("MLPayData_Total.csv")
```


## EDA: Relationship between payroll changes and performance

Payroll may relate to performance among ML Baseball teams. One possible argument is that what affects this year's performance is not this year's payroll, but the amount that payroll increased from last year. Let us look into this through EDA. 

Create increment in payroll

i. To describe the increment of payroll in each year there are several possible approaches. Take 2013 as an example:

    - option 1: diff: payroll_2013 - payroll_2012
    - option 2: log diff: log(payroll_2013) - log(payroll_2012)

```{r, echo=FALSE}
#let's look at the differences for 2013
payData$p2013 - payData$p2012
plot(payData$p2013 - payData$p2012)
log(payData$p2013) - log(payData$p2012)
plot(log(payData$p2013) - log(payData$p2012))

```

Explain why the log difference is more appropriate in this setup.
  
  We want to use the log difference as it allows us to approximate the percent
  change in the payroll rather than just the difference in pay. By using the log 
  we can compare the different teams in a more efficient manner. 

ii. Create a new variable `diff_log=log(payroll_2013) - log(payroll_2012)`. Hint: use `dplyr::lag()` function.

```{r, echo=FALSE}
#creating the diff_log function
baseball$diff_log <- log(baseball$payroll) - lag(log(baseball$payroll))
#note we set the year 1998 to NA since the row before is a different team, and
#that is the first year that we have data for
baseball$diff_log[baseball$year==1998] <- NA
```

iii. Create a long data table including: team, year, diff_log, win_pct
```{r, echo=FALSE}
#creating long table 
baseballNew <- baseball[,c("team", "year", "diff_log", "win_pct")]
```

## Exploratory questions

i. Which five teams had highest increase in their payroll between years 2010 and 2014, inclusive?
```{r, echo=FALSE}
#payroll differences between 2010 and 2014 (p2014-p2010)
#make a new data set with just 2010 and 2014
tenFourteen <- subset(baseballNew, year == 2010 | year == 2014)
tenFourteen$fourYearDiff <- tenFourteen$diff_log - lag(tenFourteen$diff_log)
i <- seq(1, 60, 2)
#we set odd numbers to NA since we don't need the differences for different teams
tenFourteen$fourYearDiff[i] <- NA
j <- order(tenFourteen$fourYearDiff, decreasing = TRUE)[1:5]
tenFourteen[j,c("team", "fourYearDiff")]
```

  The teams with the highest increase in payroll between 2010 and 2014 inclusive are
  1. Houston Astros	0.812		
  2. Oakland Athletics	0.506		
  3. Arizona Diamondbacks	0.427		
  4. San Diego Padres	0.418		
  5. Texas Rangers	0.393				


ii. Between 2010 and 2014, inclusive, which team(s) "improved" the most? That is, had the biggest percentage gain in wins?

```{r, echo=FALSE}
#determining biggest percent gain in wins
tenFourteen$pctChange <- tenFourteen$win_pct - lag(tenFourteen$win_pct)
i <- seq(1, 60, 2)
#we set odd numbers to NA since we don't need the differences for different teams
tenFourteen$pctChange[i] <- NA
j <- order(tenFourteen$pctChange, decreasing = TRUE)
tenFourteen[j,c("team", "pctChange")]
```
The team with the biggest percent change, or that improved the most from 2010 to 2014 was the Pittsburgh Pirates. Their percent change was 0.19136.


## Do log increases in payroll imply better performance? 

Is there evidence to support the hypothesis that higher increases in payroll on the log scale lead to increased performance?
Pick up a few statistics, accompanied with some data visualization, to support your answer. 


```{r}
# create average winning percentage and diff_log for each team
data_agg <-baseball %>% 
  group_by(team) %>%
  summarise(
    diff_log_avg = mean(diff_log, na.rm = TRUE), 
    win_pct_ave = mean(win_pct))
str(data_agg)
summary(data_agg)
```

```{r}
# now let's look at the averages on a plot and see if there is some relationship
#install.packages("ggrepel")
#library(ggrepel)
data_agg %>%
  ggplot(aes(x = diff_log_avg, y = win_pct_ave)) + 
  # geometric options: color, size, shape, alpha: transparency (range: 0 to 1)
  geom_point(color = "blue", size= 3, alpha = .8) + 
  geom_text_repel(aes(label = team), size = 5) +
  labs(title = "MLB Team's Win Percent  vs. Change in Payroll", 
       x = "Average Change in Payroll", 
       y = "Win_pct_ave")

cor(data_agg$diff_log_avg, data_agg$win_pct_ave)

```
Based on this graph of the average change in payroll, (average of diff_log for each team)
and the win percent average over the years 1998-2014, there does not appear to be 
any correlation between the variables. In fact the correlation is nearly zero meaning
that those two variables have little to no effect on one another.


```{r, echo=FALSE}

plot(x = baseball$diff_log, 
     y = baseball$win_pct, 
     pch  = 16,     # "point character": shape/character of points 
     cex  = 0.8,    # size
     col  = "blue", # color 
     xlab = "Percent Difference in Payroll Between Years",  # x-axis
     ylab = "Win Percent",  # y-axis 
     main = "MLB Team's Win Percent vs.Change in Payroll") # title

```

The correlation between the log difference of the payroll between years and the
win percentage for games that year is very low. 0.345 indicates there is almost no
correlation between those variables. 

We can also use the graphs from the prior question to see that the top 5 teams
with the greater percent increase in payroll between 2010 and 2014 do not 
line up with the top 5 teams that had the biggest percent change in wins from those years


```{r}
baseball %>%
  ggplot(aes(x=diff_log, y=win_pct, group = year, color=team)) +
  geom_point()+
  geom_smooth(method="lm", formula=y~x, se=F,color = "red")+
  facet_wrap(~year) + 
  theme_bw() +
  theme(legend.position = 0)
```
From the plots above, we are able to look at the changes in payroll on the log scale
and the win percentage for group by each year. We can see that there is no clear positive
correlation for all of the years. Some have a least squares line that is flat, some have
positive, and have negative. This further proves my belief that there is no relation between
change is payroll on the log scale and winning percentage. 


So overall, we can conclude that there is not evidence to support the hypothesis
that higher increases in payroll on the log scale lead to increased performance. 
In fact our data showed that there is no correlation between the payroll increases 
on the log scale and the percentage of wins per year for teams

## Comparison

Which set of factors are better explaining performance? Yearly payroll or yearly increase in payroll? What criterion is being used? 

Yearly Payroll

```{r}
baseballSubset <- subset(baseball, !is.na(diff_log))
cor(baseballSubset$payroll, baseballSubset$win_pct)
cor(baseballSubset$diff_log, baseballSubset$win_pct)
```
We can see that Yearly payroll has a higher correlation with win percentage than the
yearly increase in payroll. We can also look at these plots below to see if
there is a visual correlation as well.

```{r}
plot(x = baseballSubset$diff_log, 
     y = baseballSubset$win_pct, 
     pch  = 16,     # "point character": shape/character of points 
     cex  = 0.8,    # size
     col  = "blue", # color 
     xlab = "Perecent Difference in Payroll Between Years",  # x-axis
     ylab = "Win Percent",  # y-axis 
     main = "Win Percent vs.Change in Payroll") # title

plot(x = baseballSubset$payroll, 
     y = baseballSubset$win_pct, 
     pch  = 16,     # "point character": shape/character of points 
     cex  = 0.8,    # size
     col  = "dark green", # color 
     xlab = "Yearly Payroll",  # x-axis
     ylab = "Win Percent",  # y-axis 
     main = "Win Percent vs.Payroll") # title

```

Although neither plot indicates a strong correlation between the two variable, you
can see there is a weak positive correlation between Yearly Payroll and win percentage
compared to the other scatter plot of change in payroll and win percentage that does
not indicate any correlation. 




